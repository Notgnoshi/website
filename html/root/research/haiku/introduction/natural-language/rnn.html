<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="The use of recurrent (which abandons the markov assumption) networks in language mdoeling.">
    <title>1.d.iv. Recurrent Neural Network Models</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="https://static.agill.xyz/css/common.css">
    <link rel="shortcut icon" type="image/png" href="https://static.agill.xyz/images/favicon.png">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <!-- <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mathtex-script-type.min.js" integrity="sha384-LJ2FmexL77rmGm6SIpxq7y+XA6bkLzGZEgCywzKOZG/ws4va9fUVu2neMjvc3zdv" crossorigin="anonymous"></script> -->
    <link href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js" integrity="sha384-XhWAe6BtVcvEdS3FFKT7Mcft4HJjPqMQvi5V4YhzH9Qxw497jC13TupOEvjoIPy7" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body class="d-flex flex-column">
    <header class="bg-dark sticky-top mb-3">
        <nav class="container navbar navbar-expand-lg navbar-dark">
            <a class="navbar-brand" href="/">agill.xyz</a>
            <div class="toggleable-content text-right">
                <a href="#" class="menu-icon text-muted">
                    <span class="navbar-toggler-icon"></span>
                </a>
                <div class="trigger navbar-nav ">
                    <a class="nav-item nav-link" href="/">Home</a>
                    <a class="nav-item nav-link" href="/blog">Blog</a>
                    <a class="nav-item nav-link active" href="/research">Research</a>
                    <a class="nav-item nav-link" href="https://mc.agill.xyz">Minecraft</a>
                    <a class="text-muted p-2" href="https://github.com/Notgnoshi"><i class="fab fa-github"></i></a>
                    <a class="text-muted p-2" href="https://twitter.com/notgnoshi"><i class="fab fa-twitter"></i></a>
                    <a class="text-muted p-2" href="mailto://Notgnoshi@gmail.com"><i class="fas fa-envelope"></i></a>
                </div>
            </div>
        </nav>
    </header>
    <div class="container flex-grow-1">
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="/research/haiku">Home</a></li>
                <li class="breadcrumb-item">Introduction</li>
                <li class="breadcrumb-item">Natural Language</li>
                <li class="breadcrumb-item active" area-current="page">Recurrent Neural Network Models</li>
            </ol>
        </nav>
        <h1>1.d.iv. Recurrent Neural Network Models</h1>
        <p>For several years, Bengio's feed-forward language model shown in <a href="/research/haiku/introduction/natural-language/ffnn">Feed Forward Language Models</a> was the state-of-the-art neural network language model. Then in 2010, Tomas Mikolov <cite><a href="Mikolov2010RecurrentNN"></a></cite><cite><a href="Mikolov2011RecurrentNN"></a></cite><cite><a href="Mikolov2012ContextDR"></a></cite>proposed a recurrent neural network (RNN) architecture that made major advancements in model performance.</p>
        <p>The use of a recurrent network over the standard feed-forward models has several advantages <cite><a href="goldberg_2017"></a></cite>. Importantly, they allow representing arbitrary-length sequences as fixed-size vectors. This is of particular importance in modeling natural language, where the lengths of sentences are quite varied. RNNs also abandon the \(k\)-th order Markov assumption that Bengio's FFNN language model follows. The use of recurrent networks allows the output probabilities of a neural network language model to be conditioned on the <i>entire</i> preceding context, instead of a fixed \(k\) number of preceding tokens.</p>
        <p>Recurrent architectures are able to do by by inputting <i>a single word at a time</i> to the network, rather than a collection of \(k\)-words as in <a href="/research/haiku/introduction/natural-language/ffnn">Feed Forward Language Models</a>. Doing so allows recurrent networks to consider the past context of any words previously input to the network by storing internal context vectors, and then using the last word's context as an input to the network, in addition to the next word.</p>
        <figure class="figure w-75 d-block mx-auto">
            <img src="https://static.agill.xyz/images/research/rnnlm.svg" class="figure-img img-fluid w-100" alt="Mikolov's recurrent language model">
            <figcaption class="figure-caption text-right">Mikolov's recurrent language model</figcaption>
        </figure>
        <p>Mikolov's architecture is shown above. The word \(W(t)\) is converted to a one-hot encoding \(x(t)\), which is then concatenated with the context \(C(t-1)\) from the previous time step, and used as the input to a hidden context layer. The context layer is then fed to the next iteration's input and the current iteration's softmax output layer to output a probability distribution for the next word \(W(t+1)\). Mikolov notes that the size of the hidden context layer should be proportional to the size of the data set. This results in training times that increase more than linearly with the size of the training set. Another problem with RNN language models is that the gradients in RNN models tend to vanish and explode during training <cite><a href="jing_survey"></a></cite>.</p>
    </div>
</body>
<!-- TODO: Find the integrity hash for <em>this</em> version. -->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css" crossorigin="anonymous">

</html>
