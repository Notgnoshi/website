<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="Application of the markov assumption to statistical language models." />
        <meta name="dcterms.available" content="2020-01-24" />
        <title>1.d.ii. Markov Models</title>
        <link
            rel="stylesheet"
            href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
            integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh"
            crossorigin="anonymous"
        />
        <link rel="stylesheet" href="/css/common.css" />
        <link rel="shortcut icon" type="image/png" href="/images/favicon.png" />
        <link
            rel="stylesheet"
            href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
            integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq"
            crossorigin="anonymous"
        />
        <script
            defer
            src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
            integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
            crossorigin="anonymous"
        ></script>
        <link
            href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.css"
            rel="stylesheet"
            type="text/css"
        />
        <script
            src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"
            integrity="sha384-XhWAe6BtVcvEdS3FFKT7Mcft4HJjPqMQvi5V4YhzH9Qxw497jC13TupOEvjoIPy7"
            crossorigin="anonymous"
        ></script>
        <script language="javascript">
            const katexOpts = {
                macros: {
                    "\\norm": "\\left\\lVert#1\\right\\rVert",
                    "\\ceil": "\\left\\lceil#1\\right\\rceil",
                    "\\floor": "\\left\\lfloor#1\\right\\rfloor",
                    "\\countf": "\\operatorname{count}\\left(#1\\right)",
                    "\\entropy": "\\operatorname{H}",
                    "\\perplexity": "\\operatorname{PP}",
                    "\\model": "\\operatorname{LM}",
                    "\\softmax": "\\operatorname{softmax}",
                },
            };
        </script>
        <script
            defer
            src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
            integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
            crossorigin="anonymous"
            onload="renderMathInElement(document.body, katexOpts);"
        ></script>
    </head>

    <body class="d-flex flex-column">
        <header class="bg-dark sticky-top mb-3">
            <nav class="container navbar navbar-expand-lg navbar-dark">
                <a class="navbar-brand" href="/">agill.xyz</a>
                <div class="toggleable-content text-right">
                    <a href="#" class="menu-icon text-muted">
                        <span class="navbar-toggler-icon"></span>
                    </a>
                    <div class="trigger navbar-nav">
                        <a class="nav-item nav-link" href="/">Home</a>
                        <a class="nav-item nav-link active" href="/research">Research</a>
                        <a class="text-muted p-2" href="https://github.com/Notgnoshi"><i class="fab fa-github"></i></a>
                        <a class="text-muted p-2" href="https://twitter.com/notgnoshi"
                            ><i class="fab fa-twitter"></i
                        ></a>
                        <a class="text-muted p-2" href="mailto:Notgnoshi@gmail.com"
                            ><i class="fas fa-envelope"></i
                        ></a>
                        <a class="text-muted p-2" href="/rss.xml" target="_blank"><i class="fas fa-rss"></i></a>
                    </div>
                </div>
            </nav>
        </header>
        <div class="container flex-grow-1">
            <nav aria-label="breadcrumb">
                <ol class="breadcrumb">
                    <li class="breadcrumb-item"><a href="/research/haiku">Home</a></li>
                    <li class="breadcrumb-item">Introduction</li>
                    <li class="breadcrumb-item">Natural Language</li>
                    <li class="breadcrumb-item active" area-current="page">Markov Models</li>
                </ol>
            </nav>
            <h1>1.d.ii. Markov Models</h1>
            <p>
                A simple approach to language modeling, useful for motivating the rest of our discussion, is to use a
                Markov chain to model the probability that one word follows another in a given sequence. A key property
                of Markov models is that they are stateless, or memoryless <cite><a href="gagniuc_2017"></a></cite>.
                That is, that the probability of a transition to a future state depends only on the current state, and
                not on the history of previous states.
            </p>
            <p class="text-center">\[P(w_{i + 1} \mid w_{1:i}) \approx P(w_{i+1} \mid w_i)\]</p>
            <p>
                This means that, as stated, Markov chain models are unsuited to modeling textual data due to their lack
                of observance of the large amount of context needed to understand natural language.
            </p>
            <p>
                Traditional approaches to building language models relaxes the general requirement of retaining the full
                history of a sequence, but not as far as the Markov chain model presented in the equation above
                <cite><a href="mikolov2012statistical"></a></cite>. These traditional approaches assume the \(k\)th
                order Markov property &mdash; that the next word in a sequence depends on only the last \(k\) words of
                the sequence instead of the full \(n\) <cite><a href="goldberg_2017"></a></cite>.
            </p>
            <p class="text-center">\[P(w_{i+1} \mid w_{1:i}) \approx P(w_{i+1} \mid w_{i:i-k})\]</p>
            <p>Under this assumption, we can estimate a sequence's probability as</p>
            <p class="text-center">\[P(w_{1:n}) \approx \prod_{i=1}^n P(w_i \mid w_{i-k:i-1})\]</p>
            <p>One method of producing this estimate is to use the maximum likelihood estimate (MLE)</p>
            <p class="text-center">
                \[\hat p (w_{i+1} \mid w_{i-k:i}) = \frac{\countf{w_{i-k:i+1}}}{\countf{w_{i-k:i}}}\]
            </p>
            <p>for each subsequence \(w_{i-k:i}\) in the corpus.</p>
            <p>
                However, one limitation of this approach is its lack of creative capacity. That is, if a subsequence
                \(w_{i-k:i+1}\) was never observed in the corpus, then its estimated probability is zero
                <cite><a href="goldberg_2017"></a></cite>.
            </p>
            <p>
                At first, this does not seem to pose a problem. If a sequence was not observed in a training corpus, we
                ought not expect a language model that understands that sequence. However, due to the compositional
                nature of natural language, it is likely that there are many more sequences that make sense than there
                are sequences in the training corpus. Thus, if we want an understanding of natural language as a whole
                we must be able to extrapolate meaning from sequence we have never seen before.
            </p>
            <p>
                There are several approaches to avoiding these zero events. One family of approaches is called
                <i>smoothing</i>, where every possible sequence is provided some small probability mass. An example of
                smoothing is called <i>additive smoothing</i> <cite><a href="chen_goodman_1999"></a></cite
                ><cite><a href="goodman_2001"></a></cite> where zero probabilities are avoided by assuming that each
                event occurs at least \(\alpha\) times in addition to its observed occurrences in the corpus. The MLE
                estimate is modified as
            </p>
            <p class="text-center">
                \[\hat p (w_{i+1} \mid w_{i-k:i}) = \frac{\countf{w_{i-k:i+1}} + \alpha}{\countf{w_{i-k:i}} + \alpha
                v}\]
            </p>
            <p>
                where \(v\) is the size of the token vocabulary and \(0 \lt \alpha \leq 1\). Another family of
                approaches is using <i>back-off</i>, where if a sequence \(w_{i-k:i}\) is not observed, the model falls
                back to using \(w_{i-k-1:i}\)<cite><a href="chen_goodman_1999"></a></cite
                ><cite><a href="goodman_2001"></a></cite><cite><a href="JelMer80"></a></cite>.
            </p>
        </div>
    </body>
    <!-- TODO: Find the integrity hash for <em>this</em> version. -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css" crossorigin="anonymous" />
</html>
