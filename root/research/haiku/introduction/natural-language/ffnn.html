<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="The use of regular vanilla neural networks in language modeling" />
        <meta name="dcterms.available" content="2020-01-24" />
        <title>1.d.iii. Feed-Forward Neural Network Models</title>
        <link
            rel="stylesheet"
            href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
            integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh"
            crossorigin="anonymous"
        />
        <link rel="stylesheet" href="/css/common.css" />
        <link rel="shortcut icon" type="image/png" href="/images/favicon.png" />
        <link
            rel="stylesheet"
            href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
            integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq"
            crossorigin="anonymous"
        />
        <script
            defer
            src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
            integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
            crossorigin="anonymous"
        ></script>
        <!-- <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mathtex-script-type.min.js" integrity="sha384-LJ2FmexL77rmGm6SIpxq7y+XA6bkLzGZEgCywzKOZG/ws4va9fUVu2neMjvc3zdv" crossorigin="anonymous"></script> -->
        <link
            href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.css"
            rel="stylesheet"
            type="text/css"
        />
        <script
            src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"
            integrity="sha384-XhWAe6BtVcvEdS3FFKT7Mcft4HJjPqMQvi5V4YhzH9Qxw497jC13TupOEvjoIPy7"
            crossorigin="anonymous"
        ></script>
        <script
            defer
            src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
            integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
            crossorigin="anonymous"
            onload="renderMathInElement(document.body);"
        ></script>
    </head>

    <body class="d-flex flex-column">
        <header class="bg-dark sticky-top mb-3">
            <nav class="container navbar navbar-expand-lg navbar-dark">
                <a class="navbar-brand" href="/">agill.xyz</a>
                <div class="toggleable-content text-right">
                    <a href="#" class="menu-icon text-muted">
                        <span class="navbar-toggler-icon"></span>
                    </a>
                    <div class="trigger navbar-nav">
                        <a class="nav-item nav-link" href="/">Home</a>
                        <a class="nav-item nav-link active" href="/research">Research</a>
                        <a class="nav-item nav-link" href="/minecraft">Minecraft</a>
                        <a class="text-muted p-2" href="https://github.com/Notgnoshi"><i class="fab fa-github"></i></a>
                        <a class="text-muted p-2" href="https://twitter.com/notgnoshi"
                            ><i class="fab fa-twitter"></i
                        ></a>
                        <a class="text-muted p-2" href="mailto://Notgnoshi@gmail.com"
                            ><i class="fas fa-envelope"></i
                        ></a>
                    </div>
                </div>
            </nav>
        </header>
        <div class="container flex-grow-1">
            <nav aria-label="breadcrumb">
                <ol class="breadcrumb">
                    <li class="breadcrumb-item"><a href="/research/haiku">Home</a></li>
                    <li class="breadcrumb-item">Introduction</li>
                    <li class="breadcrumb-item">Natural Language</li>
                    <li class="breadcrumb-item active" area-current="page">Feed-Forward Neural Network Models</li>
                </ol>
            </nav>
            <h1>1.d.iii. Feed-Forward Neural Network Models</h1>
            <p>
                Using feed-forward neural networks were first seriously applied to modeling natural language by Yoshua
                Bengio in 2003 <cite><a href="bengio2003"></a></cite>. Bengio used a feed-forward network, where each
                word in the vocabulary is mapped to a \(m\)-dimensional vector in a continuous vector space.
            </p>
            <div class="alert alert-info">
                The paper <cite><a href="bengio2003"></a></cite> is light on the details of the mapping \(C : V \to
                \mathbb R^m\). <cite><a href="pappas_meyer_2012"></a></cite> fills in some of the details, but still
                doesn't explicitly explain how \(C\) is constructed. It seems to indicate that \(C\) is
                <i>learned</i> at the same time as the rest of the network? So does that means that the actual inputs
                are the word indices?
            </div>
            <p>
                Then each word in the sequence \(w_{i-k:i}\) is mapped to their corresponding vectors, which are then
                concatenated to form the \(k \cdot m\)-dimensional input vector for the neural network.
            </p>
            <figure class="figure w-75 d-block mx-auto">
                <img
                    src="/images/research/ffnnlm.svg"
                    class="figure-img img-fluid w-100"
                    alt="Bengio's feed-forward language model"
                />
                <figcaption class="figure-caption text-right">Bengio's feed-forward language model</figcaption>
            </figure>
            <p>
                The softmax output layer shown above is the most computational aspect of Bengio's language model &mdash;
                given a vocabulary of size \(v\), one softmax computation requires a matrix-vector multiplication with a
                matrix sized \(d_\text{hidden} \times v\), followed by \(v\) exponentiations. This makes using large
                vocabularies prohibitively expensive <cite><a href="goldberg_2017"></a></cite>.
            </p>
        </div>
    </body>
    <!-- TODO: Find the integrity hash for <em>this</em> version. -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css" crossorigin="anonymous" />
</html>
